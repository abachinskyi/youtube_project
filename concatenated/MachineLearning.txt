
TV3 

dear fellow Scholars this is too many papers with Carol Jean if this paper is as fresh as it gets as of the making of this video it has been out only one day and I got so excited about it that I wanted to show it to you fellows colors as soon as humanly possible because you've got to see this not so long ago we have been talking about deep neural networks the technique that was inspired by the human visual system is enabled computers to learn things in a very similar way that if you Monroe days the previous too many papers episode on this just click on the link in the description box if you've missed it you know networks are by no means perfect so do not worry don't search up yogurt some applications are getting out of control in Google deepmind case it started to learn playing simple computer games and eventually show superhuman level place in some cases I ever on this piece of gold and got some pretty sweet results that you can check out there's a link to it in the description box as well so about this paper we have here today what does this one do you take photographs with your camera and you can assign it any painting and it will apply this paintings artists start with you can add the artistic style of Vincent Van Gogh's beautiful Starry Night with and get some gorgeous results or if you're looking for read more emotional or may I say disturb look you can go for Edvard Munch The Scream for some stunning results and of course the mandatory pic so as you can see deep neural networks are capable of amazing things and we expect even more revolutionary works in the very near future thanks for watching and I'll see you next time 

Temple and Imo call Bass Pro TF card okay the book of number is 63 of 15 Phuket X text okay this TF pacifier SD Tactical music and people Baytown Texas the last one please send email to different state Tuesday OKC Thunder 

I'm Chris and Agosto blows the chief scientist and co-founder of method Innovations is a machine we specialize in machine intelligence for data in motion at Cisco live in San Diego demonstrating our main product which is focused on anomaly detection picture we have to put it on a Raspberry Pi which is a tiny computer just to emphasize how come station efficient are product its intention is that you can plug in sensors if this guy here it reads the data in real time in Phoenix up a profile abnormal behavior and is able to detect anything up normal from that home right now we replaying your life that in real time they said from a wind turbine 9000 of the patients per second are being processed in real-time here and anomalies already 10 displayed on the front end various different of monies are correlated to give an overall health metric for the wind turbine which one to use for predictive maintenance of full diagram in the first instance were focusing to EUR on a wind turbine application it didn't go to wind turbines 90 sensors producing day top 100 hurts that's a true the Deluge Ultimate Care II can bring both sides were found with convenient solution offers cutting-edge machine learning at the edge situated on the wind turbine Rising Dayton real-time cases like that include oil and gas as well as of course manufacturing and Telemetry in grade for instance and even ultimate the difference between analytics and machine learning is the transition from fixed rooms that are mostly determined by human experts with the help of data Bots Analytics self-taught rules that involved with the data and our ultimate give discovered by the machine when you have a complex device like a wind turbine operating in the highlands Dynamic environments participate in advance every possible scenario and control it using fixed Roos instead with machine learning the Arctic Sturgis able to evolve with the data and discover the best rooms for the given situation our tagline is machine intelligence in motion not really captures are Vision about the picture of artificial intelligence it's been to operate all the edge in real time where the action is helping the machines to control themselves to operate as efficiently as supposed to be then minimizing the risk of Falls and other catastrophic failures from the way that existing solution approaches problem we don't believe that data storage has value in itself we focus on the actual insights and these are only available to have a very short shelf life so they're only tell me if you can extract them in real-time which is what we focus on but there are several different angles that make our product unique one of the detritus but you can operate on what happens if I drink since then private since the data like Hector de the princess the note ever need to leave the device similar in Orland gas Securities another transferring data is one of the biggest issues raised right now so instead we bring the analytics to the edge 

hello here I'm going to the dumbest video of very simple fpga implementation of Kayon algorithm this is Cellairis in PAC Rhythm although it is very simple it is capable of face detection and handwriting detection in Madison area and we have already seen Star machine learning so cute in this fpga development board and that is our camera the camera will capture the image my face and it will show you the image it captured in the left block of The BJs Kreem and do I give a prediction and show the prediction in the right Glock of the Vigilant and now let's begin with the virus remind to let the machine learning sake to distinguish my face and this thing space so resume you see the laptop lock is my face captured by the camera and The Rock in the right lock is prediction the prediction is always my face and hurricanes always and most of nearest picture in the database from the image that the camera capture change to his face to see in real time the circuit distinguished that did this thing space change it back even our trash and girl and you sleeping face expression as it's really very low error rates and 2 from match match faster than CPU implementation because this is Tyler computing fpg architecture you can you can sing this as a very large scare Circle that can do a lot calculation computation in just one or two that is worse if you cannot do very hot very fast almost in Rio 

but when it comes to Advanced analytics what customers really want and what he bows down to is pretty currency if you're at Lenox is faster more skilled and more accurate then you can pay her you'll have a clear quantifiable advantage in the marketplace today machine learning is actually all around us every time you search for something on the Internet it's machine-learning that's figuring out what adds to show you every time you buy a turn line probably there's machine learning recommending other products to you where do you get the highest and performance and machine learning that's the void that's guy Ray's filling what is trying to predict tomorrow's stock price whether you're trying to make a recommendation on whatever part of you should buy even a small difference call Chris he can make the difference between failure and success in the 20 years since I started doing data analysis from my point of view nothing really has changed and the reason turn that is that it's an extremely difficult problem it has aspects of state-of-the-art statistics and state-of-the-art those are just two very different kinds of fields and it's Skytree we've put the two together so customers range from the largest organizations on the planet all the way two companies with simply going to have an answer in the marketplace whether they are drowning in there whether you're more fast scalable more accurate approach do without latex those are the Caps application and seeing out there and machine learning engine it does the heavy lifting of powerful machine learning algorithms it sits between your data source and your front-end will users are data scientist these are the people who collect the data turn the data and finally make machine learning models to find patterns in the data and make predictions in the modeling face you want the highest performing statistical model and in the production phase you are often constrained by computation and you want the highest and computation performance in Skytree is the only system that was really designed for production machine money but if using a lileks Linn organization play you want to know are we here to ride softer and develop code or are we riding or creating analytics models if it's the latter then scatter the clear all ternative because you can run out of the box and give all your models forces writing out of them to self in a few years no matter what your organs ocean no matter what's your field or industry if you're not doing machine learning your competitors will be we can now make predictions with a high degree of accuracy and confidence 


hi there my name is Vincent together a chill I will guide you through your first steps and world of machine learning in the previous video you learned about tree machine learning classification regression and clustering to my default there are quite some similarities between classification and regression for both you tried to find a function or remodel which and later be used to predict labels or values for unseen observations is important that you bring the training of the function labeled observations are available did algorithm we call these techniques supervised learning labeling can be a tedious work and it is often done by humans therap other techniques which don't require labeled observations these techniques are called unsupervised learning what are the acquainted yourself with one of these techniques in the previous video namely clustering clustering will find groups of observations there are similar and US does not require specific labeled observations in the next chapter will talk about assessing the performance Usher train model and supervised learning we can use a real labels of observations and compare them with the label sweeper it's quite straightforward that you want your models predictions to be a similar as possible to the Real Deals unsupervised learning however measuring performance gets more difficult we don't have any real labels to compare anything to you learn some new techniques to says the quality of a clustering in the next chapter as you get more experienced as data scientists might notice that things aren't always black and white machine learning some techniques overlap between supervised and unsupervised learning with semi supervised learning for example you can have a lot of observations are not labeled and if you which are you can dance first perform clustering to group all observations which are similar after words you can use information about the Clusters and about a few label observations to assign a class unlabeled observations this will give you more labeled observations to perform supervised learning on enough talking do some exercises 

hi my name is Timo Elliott I'm an innovation evangelist with ASAP I'm here at Sapphire now with David Judge David tell me a little about yourself yes so I just very recently joined sap and I've joined as the cheat evangelist for pick of analytics first and now as you've seen the launch announcement for Leonardo will be working quite a bit on that as well cursive it about your background why what's your background and what brought you to sap why did you want to so I have about teen years now within machine learning cognitive Technologies I think we're playing buzzword bingo earlier today so my background has that littered throughout it and being an only a month into a CP I think I've been extremely impressed by never won the quality of the individuals through within the business and I got brought in by a former by a friend of mine so I had some clue about that but also the enormous opportunity that we have statistic was 350,000 businesses have that say p as a system of record which means if we start layering some of these disruptive Technologies on that digital core we've got miles to go and lots of customers to help just imagine customers watching us now they like okay what are the how to make a zillion times what's in it for me can you make it more real so I think one of the things to keep in mind about machine learning is it it's not something that you do on to it best to be applied to a business process it's a fruit sample if you can look at the way that a customer might interact with your business from the outside cognitive Technologies like chatbot when something has been quite common right now so that's one element of machine learning but then looking more around even structured data where we can convert special use cases of Predictive Analytics all those things coming together to close the front of the office being with the back of the office and make things happen a lot faster should be doing as part of your Innovation wrong I'll be working with customers Partners certainly a fair number of speaking engagements most likely and then helping to spread sort of the pragmatic vision of how Leonardo on might give to help customers as well now where most of the way through the first day what are your impressions been it's been pretty welcome aboard and thank your time 

everything in your organization produces data but can you transform it into reliable intelligence with machine learn can machine learning gives computers the ability to learn without specific programming adding powerful tools that require less human effort model to become simpler predictions improved through continuous model retraining forecast get smarter over time this revolutionary tool is trans Industries empowering autonomous vehicles personalized Healthcare in real-time sales forecasting it helps Banks prevent fraud scientists achieve and utilities improve reliability organizations can identify unique Market needs and behavioral patterns and move with agility is your biggest challenges are your best opportunities you can seize them with IBM machine learning 

hello and this video will be covering the reliability of I'm Forrest we said before that a decision tree may not produce the optimal tree since it uses a greedy algorithm to build it however there is a sense of reliability that comes from the capacity of random Forest to find the optimal tree this is done by randomizing the data sets creates each trade where there is no overlap and data then a data point is pushed through each tree where the result is the average of the results straight slussen what that would look like here we have a data set consisting of four colors red blue green and yellow instead of creating one decision tree for this data will split up the data set the split would be random but for the sake of easy visualization this here have been made evenly the data said has been split into four subsets as such for decision trees can be built in parallel since these trees are being built at the same time it decreases the runtime to build the trees since were building for small trees rather than one large tree next we'll insert a data point into each tree the point will Traverse through the tree until it reaches the leaf node looking at where which point is located in the respect of trees think about what color each one should be based on the other colors in that note let's take a look at the or class prediction of what each point would be so based on all of these histograms we need to average them too for real classification of the unknown data point this istagram is the average of the results of the four trees as you might expect this simply me that we took the results of each color out of them together and divided by the number of trees so this is what the unknown data point can equal we see that the average presents a highest chance of being yellow with green being a close second clearly there's also a very low chance of the point being or blue there are two sources of Randomness in random Forest first there is Randomness in the data due to bootstrapping and how can there is Randomness in the split of the features Randomness important in random Forest because it allows us to have distinct different trees that are used off of different data this is important because if we push a data point through each tree and they all produce the same result the most likely that day play can be classified as that result none of the trees have any association with any other tree due to the uniqueness decision tree you're very deep tend to over fit the data where they have low bias and high variance when we have each tree based on different subsets of the data tree ends up having a lot higher variance by averaging these trees were able to reduce the overall variant although there's still a small increase in bye another reason why bootstrapping is good is that we're able to better the models performance by preserving the bias while decreasing the variance thanks for watching 

so what's up I'm just waiting for my luggage and then I told why no I just make a quick answer we go because I have like more than 1,000 people are question I really need to care time now yes I'm behind schedule you people are asking me questions or machine learning nanodegree would you take a PhD I mean I had what prosecutor security tag Google company look like letting go and get a PhD for machine learning nanodegree is for those who are ready to be who are ready in like a sir coming in machine learning Applied Mathematics complex Elva which you need to sort is evil even you think you can become a machine engineer without PS2 Mother Dan you need at least 5 to 10 years experience in your field say you need one paper published I mean International Paper if you cannot hand is really very hard on me I want their commanding on me I think I you that ticket driving and reading Indian rupee which I totally do not recommendable say I may look into another like you in jail for like that of which is now not anymore are you turning point of the Pacific ATM machine learning machine any computer similar to my wife is doing right here near Little Rock fish taste like fruit are healthy Independence answer questions are too many questions 

welcome to the machine and tool box scores I'm asking the statistician and author the carrot package which I've been working on for over a decade take care to the most Wiley's packages and are for supervised learning also known as predicted by supervised learning is machine learning when you have a Target variable for some specificity want to predict the classic example of supervised learning is predicting which species and I are Siz based on his physical measurements another example be predicting which customers in your business will churn or cancel their service in both these cases we have something specific that we want to predict on new data species in sure there's two main predictive models classification of regression classification models predict quality the various pieces of a flower or will a customer. regression models predict quantitative variables example the price of a diamond once we have the model we can jam metric to evaluate how well the model works imetrik is quantifiable make us an objective measure of how will the model predicts on your data progression problems will focus on the root mean squared error or rmse is are metric of choice does the air that linear regression model seek to minimize for Zeppelin the LM function in R it's a good general-purpose error metric and one of the most common ones were Gresham fortunately it's common practice to calculate root mean squared error on the same day that we used to fit the model this typically leads to over optimistic estimates of model performance this is also known as overfitting a better approaches to use an outer sample estimate for model performance Disney purse care takes because it's simulates what happens real world and helps us avoid everything however is she so stirred up by looking at in Sample error so we can then contrasted later with Adam sample error on the same data set first we load the empty cars data set and fit a model to the first 20 rows next we make an ensample prediction using the predict function on our model finally we calculate the root mean Square on our training data get pretty good results now let's practice calculating root mean square error on some other dates 

so this is going to lead us to the concept of expectation maximization so expectation maximization is actually at an algorithmic level it's surprisingly similar to K means so what we're going to if you're going to tick tock back and forth between two different probably list to calculations so you see that it kind of Drew it like the other one the names of the two phases are expectation and maximization sort of you know our name is our algorithm so what they're going to do is we're going to move back and forth between a soft clustering and Computing the means from the soft cluster so the soft clustering goes like this this prophecy indicator very Rosy IJ represents the likelihood that data element I comes from cluster J and so the we were going to do that since for the maximum likelihood setting is to use Bayes rule say well that's going to be proportional to the probability that data element I was produced by cluster J and then we have a normalization factor normally we don't have the prior in there too God Charles because you said it was a maximum likelihood it's not going to have any impact normalization so that's what this Estep is if we had the Clusters if we knew where the means were then we could compute How likely it is that the day that we come from the means and that's just this calculation so that's Computing the expectation defining the variables from the mute the center's we're going to pass that information that cluster Nations Z over to the maximization step what the max raise the temp is going to say is okay well if that's the clustering we can compute the means from those Custer's all we have to do is just take the average variable value Rite soda where to the excise within each cluster J what's the likelihood it came from cluster J and then again we have to normalize if you think of this as being a 01 indicator very then really it is just the average of the things we assigned to that claster but here we actually are kind of soft assigning so we can have half of one of the data points in there and only counts half towards the average we can have a tense in one another place and a whole value in another place and so we're just doing this weighted average of the data points listens to me and I cannot even get that for the gas in case the zi variable always be nonzero in the end because there's always some probability that it comes from some couches to have infinite extensions to me is there a way to take exactly this algorithm and turn it into K means I'm staring at it and it feels like if all your probabilities were ones in Rose you would end up with exactly kamin think when the maximization step would be the means which is what K means does then what would happen we send these means back and what we do and Kami just we say data point belongs to its closest Center which is very similar actually to what this does accept that here we then make portional so I guess it would be exactly that if we made these clustering assignments push them 201 depending on which was the most likely claster the probability to be into a cluster actually depends upon all the Clusters and you always got a one or zero Basie did this is like a hidden art Max kind of thing or hidden Max or something then you would end up with exactly came in like is improving in the error metric this squared error metric this is actually going to be improving in a probabilistic metric right that the data is going to be more more likely that makes sense 

in summary we had them for different made of blocks of techniques you try to extract the information in the form of a features you feed those features into a machine learning algorithm this is the heart of machine learning and then you evaluate that entire process to see how good you've done one of the hardest who's is actually coming with the data set or the question that interest you so one example that I loved was the Enron data set for you this will be something different you're spending how some features how to present features how to get rid of features notify new feature spaces it actually work better and you can apply these techniques to pretty much a data set really reasonable feature sets were the items were the most funnest machine learning you know that they're supervised if you have labels your labels more classes machine and we can talk about but for the sake of this class there's movie toward you and we didn't Porton this each other with him needs to be too we talked about quite a bit of the two new items there's different types that you can what's most important to you and at this point once you've worked your way through from beginning to end the evaluation metric will tell you if you want to go back and revisit any of these steps or if you're happy with your results and you're ready to move on this is the entire class on one slide is not amazing 
a machine does nothing aside from what you tell it to do so you have to teach a machine everything that you want it to do including how to learn if you want to teach a machine what's an apple you have to show it a lot of pictures of an apple it's getting what are the key features of what makes an apple and Apple find where all the solar panels are in the u.s. and weed to write some kind of algorithm that I look through all the images of the u.s. what's going on what's not and trained and tested out of her than we need to show the room what existing pictures of Softails look like so right now we're doing that process we were looking is it ourselves funny solar panels clicking around the vertices of the solar panel and saving those images tedious it's not necessarily something that's pointless right now the US estimates of soul energy capacity especially rooftop solar PV are highly limited to any type of self-reported information or other government records we're trying to do here is to actually detect the images of the solar panels using machine learning I can find out how many solar panels are actually in use like generating energy in the u.s. today would be really awesome this is government regulators and for policymakers to understand how much solar has been deployed Philip evening this week is going and this kind of data can be very useful for policy decisions later on so I just took my first and it's really helpful for his project I like do big data problems in the real world so the chance to like see a real world problem do some real world research using my thanks feels is necessary for engineers to learn big data and data science big data will transform or make a lot of Engineers lives easier but big data by itself will not help anyone what you need is a bility to transformed into usable information for an engineer's and scientists that are graduating now having strong skills and in data analysis we really keep going Maury 
this class is divided into three subclasses three parts they are supervised learn unsupervised learning and reinforcement learning what do you think supervised learning is so I think of supervised learning as being the problem of taking label data sets gleaning information metsa that you can label nudists and I'm going to use pears and I want you to guess with the functioning okay okay 1124 hang on his one put in one the output yes into the input for the output correct alright I think I'm onto you text 749 this is exactly maybe so if you believe that's true can tell me if the input is 10 what's the output and that's right in fact that the function is but the truth is we have no idea where the not really you do what is that idea come from 10 + map well I'm going to talk to me for a long time I'm going to claim that you're making a leap of faith despite being a scientist by deciding that the input is Tan in the output is 100 degree with that what's that well I mean from what you told me it still consistent with lots of other mappings from input output like 10 gets map to 11 everything is I think you very much I will saving up where to accept everything squared up to 10 

turn here skip song sausage chair cushion 3 the kitchen train your Betta cushion issues in America Yahoo Yahoo 

always most of been talking about that the training date I'm right so and the classifier of the algorithms use the training data to build the predictors so that's basically pairs of instances attribute sex I along with some targets why I which could be a client class or number or something so that's what we used to train to build a model building reason we're we're creating does that sometime tomorrow or in the future we going to get new dates up and data we're just going to have the app what instance is themselves and we will not know the true the true labels so we want to use our classifier or our regression system to print those values why I for the day time and where you want to do row on so it doesn't it doesn't matter how well your Plaza fire does on the training data because there's no point ride for the training date are you already know the outfits as many classified as you can predict them arbitrarily well so this isn't trees yet you can get a hundred percent accuracy them nearest neighbor you can get a hundred percent on the training date that's not interesting what's interesting is it going to work tomorrow how's it going to work on the data that you haven't not that you haven't seen before 

greetings my name is Matt would and I'm here to give you a quick update on Amazon machine learning we have can you features available to all customers for Amazon machine learning the ability to shuffle your training data for improved model quality Amazon machine learning better morals when input rows are presented to the algorithm and random order through this new training parameters option you can have Amazon machine learning Shuffle the order of your rows of your data automatically and can be controlled by the SDK just currently stored in Amazon redshift by using the method I turn red ship tables to populate the Moto training data schema in Amazon machine learning less manual it's more that's it two new features for Amazon machine learning available today thanks a lot 

so the characteristic of informal learning is first dance that you take a bunch of simple rules all of which kind of makes sense and you can see it sort of helping but turn individually do not give you a good answer and then you magically combined them in some way to create a more complex rule then if act works really well and learning algorithms have a sort of basic form to them that can be described and just one or two lines so let me do that and then we can start wondering a little bit how we're going to make that real so there's a basin formed Ensemble learning out Riddim basically you learn over a subset of the data and that generates some kind of a rule and then you learn over another subset of the date and that generates a different Rule and then you learn over are subsets of the day that generates yet a third Rule and you the fourth rule the fifth Rule and so on and so forth and eventually you take all of those rules and you combine them into one of these complex rules so we might imagine in the email case that I might look at a small subset of email that I know is already spam and discover that the word manly shows up in put them and therefore pick that up as a rule that's going to be good at that subset of male but not necessarily be good at the other subsets of mail and do the same thing and discovered who are the spam males are in fact short or a lot of them are just images or just URLs and so on and so forth and that's all I learn these rules by looking at different subsets which is why you end up with rules that are very good at a small set of the data but aren't necessarily good had a large said the data and then have you collected his rules you can bind them in some way and there and that's really the beginning in the end of in Samba morning till Wait So when you say manly was in a lot of the positive examples do you mean like it distinguishes the positive negative example so should also not be in the negative example that's right that's exactly right so that you would have with some way to distinguish the positive in the negative and why are we looking at I understand why we can't just look at the whole data we look at all of the data then it's going to be hard to come up with Simple Rules that's the basic answer ask me that question a little bit later when we talk about over fitting and I think I'll have a good answer for you okay so here we go Michael what's on maligning you learn over a subset of the data over and over again picking up new rules and then you can buy in them and you're done 

well what you doing or do you make that work and what you end up doing a supervised learning and function approximation in general is you make some fun the middle of some shins about the world you decide that you have well behaved that is consistent with the data that you're giving and with that you're able to generalize in the fact that is the fundamental problem in machine learning it is generalization behind all of this I'm going to claim Michael you jump in whenever you disagree it's a particular inductive bias so all of the shame are certainly supervised learning is about induction is supposed to deduction induction of course being the problem of going from apples to a more general rule specific instances basically like reasoning a lot of in the beginning was about to doctor reasoning about logic programming those sorts of things and you deduce only those things are following me Lea from those rules so for example you have something like a implies be that's a rule in the universe and then I tell you a play Empires V is a rule the universe and I tell you and you also know am pies be and therefore you can and end for that be big you have any plans me what we just did was not deduction before then when I asked you 1 1 2 4 3 9 4 16 and so on so forth right we did induction did the Sun rise yesterday did the sunrise the day before that so the sun is risen everyday is the Sun going to rise tomorrow we all hope so and we all act like it does if it doesn't there other things sitting in the studio having Center in the plants 30 there is induction is crucial and the inducted bicycle will talk about all this in the course that's kind of fun of emotion behind supervised learning in machine supervised learning tell me with some function that generalize is Bianna dated Egyptian induction then go someone's a supervised learning first said surprise learning is function approximation and I want to say why don't you say supervised learning is function induction supposed to function approximation approximate function induction or induction of approximate of approximate functions yeah but sometimes you can sometimes you think it's quadratic but it's not I had that as a plaque on my wall you do know okay so that supervised learning we own more than 200 film for sale Medical Center's identified that there was need to provide Quality Healthcare and affordable prices from the earliest days of Fullerton was itea key factor processes the key thing is to get information on the same day immediately after patient has been delivered to the patient and if we can is a business get Nation about that transaction making the date on my nipple and all to let me having this is intelligence that I should leave predicts and actually maybe even recommend the next best cost of action to give me evidence-based information they will allow me to shape after nickel practice starting to do more with Microsoft top fraudulent customer service for the cost of Performing healthcare service about a year-and-a-half ago we decided on a cloud for mobile first Philosophy for RIT machine earnings going spotting last 10 phone lines are not able to do stuff in data analytics that we are not able to do five years ago to example with military particular client at three thousand lives and they had what percent of population with chronic illnesses are the tension high cholesterol diabetes related consumer 70% cream civilization in the full year for that particular employer in what region proposed to do because of our data analytics first to come out with a crying disease management program for this population are you drawn by 60% because we were able to manage the point of this is better which resulted in fewer hospitalizations and no absences indecline was delighted Fullerton health is using these technology savings to not only bring down the cost of providing health care but the company spearheaded a program old project big heart and a privileged position as a medical organization I think we felt that giving back Medical Services was probably the best play Miku contribute the greatest resources the thousands of healthcare workers that we have across five countries today in the women do to Beccles that have a medical project in every single country division where we will reach out to the underprivileged employees share in the project at least once a year did the big events or at the clinic provide free medication to treat chronic you could be diabetes or hypertension and so forth with a commitment to the community here that as long as bulletin is around we will continue to support this 

at this point in the class you've learned a lot about many of the different moving pieces that might go into a machine learning analysis and I think it's worth taking a step back explicitly talking about where you want to use your training data and where you want to be using your testing data cuz this can be a little bit confusing sometimes suppose put your overall analysis flow looked something like this you start out by splitting your overall data into training and testing sets then the next thing that you want to do apply PCA so a future transform and then you want to take a few of your leading principal components and put them into an svm a classification Rhythm remember that PCA itself can have a few different commands that you might call on it so you can call fit on your PCA which actually when's the principal components of your data but then you also have to transform which uses this fits found to actually transform the data the new principal components representation likewise a support Vector machine or a support Vector classifier can be fit on some of the data and then be used to make predictions on a different set of data suppose that coming out of the training testing split we have two different sets of features we're only going to look at the in this quits we have with called training features and test features the first thing you want to do is apply principal components specifically you want to find the principal components using the command do you want to apply the PCA. Fit to the training features or to the testing features 

first there was the mind then the machine in the dawn of artificial intelligence and machine learning I think the machine is gaining ground on the mind but only with your vision will machine learning reach its full potential the strength of machine and mind now come together for you on the sap Leonardo machine Learning Foundation with ready so intelligent Business Services like service ticketing and functional services using machine learning building are the intelligent processing of all your data to detect to recognize to classify to predict and more sap Leonardo machine-learning runs on sap clown plant do unable all applications in your existing landscape to become intelligent so you can create your intelligent Enterprise and sign machine learning applications quickly and easily it supports a strong ecosystem of Partners and developers using CP Leonardo machine Learning Foundation which continuously adds value to join customers you have the imagination now that allergy is here to bring it to Life as a PE Leonardo machine Learning Foundation 

so Katie this is going to be a unit on unsupervised learning unsupervised learning is something that's very important because most of the time the day that you get in the real world does have little Flags attach the tell you the correct answer so what are you to do as a machine Learner in that case you turn to unsupervised techniques to still figure something out about that data okay let's talk about them given a dataset without labels over all the data points are off the same class that I'm still thinks you can do to extract useful information like this day to sit over here where I would say this data set is structured in a way that is useful to recognize for machine when we look at this by it looks like there's clumps or clusters in the data and if we could identify those clumps are clusters we could maybe do you say something about a new unknown data point and what its neighbors might be like what is a second example of data may be the day that looks just like this that's all you can say Isabel all the data in this example looks like it lives on some kind of line or some complicated shape that your seem to be drawing in there right now is it's used to be a two dimensional space with X and Y over here but some of you can reduce it to a one-dimensional lions so it's called what that's called dimensionality reduction now detection clustering clustering in this lesson an example of cloudy reaction in so-called unsupervised learning you're not going to dive into the wonderful land of a magical land of unsupervised learning 

hello and welcome to machine learning 101 in this course will be reviewing two main components first you'll be learning about the purpose of machine learning and where it applies in the real world chicken you'll get a general overview of machine learning topics such as statistical modeling supervised vs. unsupervised learning model evaluation and machine learning algorithms and clustering there are many technologies that integrate machine learning many of which you may use daily life a great example of this is Snapchat in which its facial recognition is an example of machine learning machine learning is also used in Xbox Kinect which projection infrared grid to determine depth movement and body shape of one or more people gesture recognition is used to n is that data and I'll put the results into the game machine learning impact Society in a very influential way here are some real life examples how do you think Netflix and YouTube recommend videos movies and TV shows to their users they use machine learning to produce suggestions that might enjoy this is similar to how your friends might recommend a show to you based on other shows you watched IBM Watson uses machine learning algorithm do almost anything including things like helping doctors identify Cancer Treatments it's also been used in developing early childhood education models there's virtually no end to what IBM Watson can do thanks to machine learning together data is gathered to train a machine learning model so we can understand patterns within the data once the model has been trained it can be used to predict the results of out of sample data or data and wish the results are known collectively this is how machine learning is achieved so now that you have a sense of what's in store on this journey let's get started with machine Barney thanks for watching and remember by completing the course you'll be one step closer to earning a badge with all the fats that come with it 

you may not be charged fairly that's a novel concept today's customer demand more value for their money not necessarily at the lowest cost but at the some companies have business models that are difficult to change and adjust to these expectations so machine-learning may come to the rescue take an auto insurance customer today his premium is calculated on a fixed amount for the Holier averaging out risks for him and his Auto for that Holier in reality however risks change in real time when is car is parked in the garage chances of an accident or a catastrophic is small when is driving on a crowded two-lane Highway his risk is high so what's the big deal insurance companies average out the actual risk the company estimates an average that's high that is making more money than it should if it estimates an average wrist that slow then it's losing more money and may shut down soon the ideal situation is when the premium changes with the risk with some buffer built-in for operations and profitable shut the closer the premium fall of the risk curve the better it is for the customer and for the company with today's technology this is possible to achieve simple GPS unit on a car can track it speed and location if every location in the country had and auto risk Valu then you can essentially match the car's location do the geography location to determine the real-time rest as the car passes from say the suburbs to the city the risk will continue to so should the premium as the drive happens this way both insurance company and the customers have a fair deal the premium for an object is simply proportional to the risk which in turn is the function of the cost of the object the controller in this case that happens to be the driver of that object and the location so how do we find these let's go back to the car the cost of the car is well known so no problem there companies already have a lot of information on their customers information like their age address number of accidents severity of such accidents Etc which component based on the person does not change from instant to instant so I'm going to see him that the risk is gradually reduces as the driver say matures and learns more accident theft catastrophic data for all the areas of the country's available partially with the insurance company themselves and partially with the federal and state isation's combining all this data it's possible to construct a risk Contour as the car's location changes using machine learning algorithms the great thing about machine learning is that the more data you feed it the more accurate it will be over time you can also adjust to long-term trends ignoring small local variations insurance company that can get to this kind of a near real-time risk calculator launch I have a huge competitive Advantage because earlier they get into the game the more time they will have to fine-tune their models to get a more accurate premium calculation some point in the future your premium can actually be proportional to your risk for listening 

so here's an algorithm that you'd probably either already know about or would be able to come up with rather quickly which is the idea of hill climbing so if this is the function that we're trying to find the Maxima then one thing we could do is Imagine just guessing some acts say which has some particular f of x value and then to say okay well let's move around in a neighborhood around that point and see where we can go that would actually improve the so we hear the neighborhood might be in a little to the left a little bit to the right on the x-axis and what we find is in One Direction it's going down the other directions going Subway hill climbing says is fine the neighbor that has the largest function value this is steepest Ascent hill climbing and if that neighbor is above where we are now has a higher function value than we are now then move to that point otherwise we stop cuz we're at a local Optima so this is going to do is just going to iterate moving up and up and up and up this curve always in a better and better and better Direction until it hits this peak here then it's going to look on sides of it the neighborhood everything in the neighborhood is worse so it just stops there and this is the x that it returned which is you know not a bad answer best answer but it's a good answer what if you'd started out as a little bit more to the left so now the other side of that Valley oh like here yeah so then well let's see what would it what would it do we start there we take a step we say what's the neighborhood the neighborhood are the points just a little bit to the left and just told it to the right one of them is an improvement so it takes the Improvement and again it keeps finding more and more improved in the this top of this little bump and it says okay both the points in my neighborhood are worse than where I am now hooray I'm done so that's not even the worst of the local opt you could actually get stuck here as well which is even lower you can speak exactly 

understand what parameters I have available to me going back to the SK learned documentation so that should look kind of familiar we found this through Google remember and I'm going to scroll little bit until I find the decision tree classifier this being documented here and I'm just going to click on this link and this takes me to the specific page this about that century classifier and this this box right here is what I'm looking for this is going to tell me all of the parameters that I have available to me when I'm creating my classifier so I can see that there's a whole lot that's going on here that I could tune there's something called the Criterion there's a splitter there's Max there's minimum number of samples in the split minimum samples in the leaf and so on and so on so these are all things that I can try tuning show me pick one of these were going to look at the men's sample split we can see if by training the Min sample split maybe we can alleviate some of that over that we were seeing in the example that I just showed you so here's what men sample split does say I have my decision tree start out with a bunch of examples then I start to split them into smaller sub samples at some point I have to figure out if I'm going to keep splitting any further so for example if I start out with a hundred examples here then maybe I get 60 and 40 then maybe I what's 40 + 20 in the maybe this 20 turns into 15 and 5 and so on and so on question is for each one of these bottommost sort of layers in the tree where I want to keep splitting it if it seems like that might be a good idea and that's what sample split governs is basically whether I can keep splitting or not whether there's enough samples that are available to meet to continue to split further so the default perform in Sample split it's too so all of these notes I can further split except for one of them so my question for you is which of the notes Here If You your choices would I not be allowed to split any further 

okay we are here in the Google self-driving car inside of the driving itself and it's doing everything by itself and I am here as a passenger I'm really hope what Sebastian did a good job at training this because he is not driving right now and tell me about self-driving cars while we're going to start by talking about supervised classification and self-driving cars are one big supervised classification for supervised start of the correct answer in those examples so I know that you have an example of this for the self-driving cars set it off again challenge we would take it off of Spin and it would very carefully watch us you and drive us drive and would emulate our Behavior what was a child I wash my pants do I have in them machine learning to give them lots of examples and they start to figure out what's going on and test out whether you can make a car and gold fast and slow at the appropriate time using machine learning supervised learning classification problem that was very important for Stanley you want to introduce. how fast you running risk of flipping over and destroying yourself so we train the car to do is to be slow down the appropriate time we did is not by any rules domestic RV drive and emulators how many miles do you have to drive to train that play smart for grad students it was very obvious that I would punch that sounds great so I think we should probably get started with that list try out a few different supervised classification problems sounds great 

my name is Jim MicroSociety University Research fellow here the University of Hartford Cheer I'm an astrophysicist my research specialty is Galaxy Felician in my hope is to try and understand how will the galaxies formed in the universe from The Big Bang for 14 billion years ago to the present day kissing I'm trying to do at the moment is develop the machine learning algorithm so unintelligent algorithm the candles Massacre identify features in astronomical imaging I can explain it is with an analogy imagine you're an alien and you come down to us and you're presented with a fruit bowl and fruit bowl contains now being an alien you don't know what those fruits are exotic to you but you have to recognize that the bananas at one thing images of something else you be at the group as different fruits according to how they look because they have different features to the bananas yellow and curved oranges orange and Ryan that's how I were the works it looks at features with an images and groups features to the very similar and by doing that it can separate parts of the image the most magic identify features that way longer than working and if Roxy feel if it one particular I'm really interested in is coolest Vehicles that's the technology that's probably going in tonight in the next 10-20 years with some way off that practically the moment one of the big challenges is most Mastic navigation that's reliable I think I'll grand less potential to act as a computer navigator on driverless cars that can just seen a hat what's not to like 10 5 features in it trees not post the road itself other hazards and not be told what to look for machine can learn on its own 

the sweatshop 

Big Data machine learning both are considered essential elements in the evolution of the modern digital workplace but what big data and machine learning mean to you how can we better support your work for us to engage with the applications processes and data available to them the reality is that they are much more than just simple Trends or technological buzzwords they are the tools that are sparking a new Revolution for how the digital work spell engage with it give intelligent tools with self learning capabilities that can augment work tasks to drive efficiency then better decision May deliver introducing these Cutting Edge tools into traditional Enterprise applications can be a challenge many organizations lack the skills and knowledge Cecily integrate Big Data and machine learning into their environments at Sunview we are solving this problem by you traducing is intelligence that leverages the latest advances in artificial intelligence to power a new generation of it we are packaging the complexity of big machine learning into a technology that seamlessly integrates intelligent features into our service management applications some of the contending technology continuously learns from an organization's existing data pen user interactions to deliver smarter more relevant Solutions the smart features include recommendation engines that provide staff with real-time suggestions for faster resolutions application Bots that part of me family everyday tasks like responding to service request intelligence search that Asher and users get the answers they need in a Self Service app Predictive Analytics that anticipate user sentiment to improve customer service and more these features add a layer of intelligent automation to support complex decision-making driving an unprecedented level of Staff productivity while empowering and users with smarter self service Smart Technologies are reshaping how we work with data and applications Sunview is equipping IT staff with the tools to make for decisions so they can take on higher-value tasks of Tomorrow the future rust system intelligence is here Sunview software powering smarter it 

this is the cancer example for my last Unit C there's a specific cancer that occurs in 1% of the population in the test for this cancer and with 9 some chance is positive if you have this cancer that's usually called the sensitivity of a test but the tests what time is positive even if you don't have see it say with another 90% chance it is negative if you don't have this usually called the space CT question without further symptoms you take the test and the test comes back as positive what do you think is not the probability of having that specific type of cancer that straw diagram suppose these are all the people and some of them exactly 1% have cancer 99% is cancer-free we know there's a test that if you have cancer correctly this diagnosis I have some chance so if you draw the area with the test is positive cancer and test Plaza send this area over here is 90% of the cancer Circle whoever that isn't the truth the test centers is positive even if the person doesn't have cancer infecting our case that happened to be in 10% of all cases so we have to add is this biggest 10% of his large area that has biggest 10% of this large area with the test might go positive but the person doesn't have cancer so this blue area is 10% of all the area over here - does small cancer Circle all the outside the circles personal situation of no cancer and a test is negative let me ask you again suppose you have a positive test what do you think with a prior probability of cancer 1% sensitivity and specificity of 90% to think you knew chances are now 90% but 8% or just 1% 

I chose Machining as my subject area when I was an undergrad student division boss it will should be smart and they should be able to learn it was the most exciting thing at the positive machinist used Vape basically import email and you can guess Netflix finds about movies to recommend using other people that looks like you and learn from those with movies daylight machine learning can be applied to almost any fear of Business Health Care education manufacturing all those Industries and waking up right now so what they need is changing engineer top them get value out of the decoding skills can become engineers less important coding and reporting that come join me in the fascinating world of machine 

it's great to learn about base Rule and what are the things for you space to in a lot for his learning from documents or text learning message to tell you about is often called naive Bayes same is no Mars not to be naive not as naive as pays for itself but that's the name okay so you going to exercise this later in our practice exams using Enron email data since but I give you the gist of it to people what is called images quotes Arizona contain three words they contain the word love the word deal and the word life same is true about Sarah office Lee people use more than 3 but this area has labored small for thirty thousand words the difference of these two people Chris and Sarah is in the frequency in which she use those words and just for Simplicity Etsy Chris loves talking about deals so 80% of his words 0.8 ordeal and he talks about life and love quite a bit with point one probabilities Chris at the warden email he's going to 80% of a Time use of ordeal and 10% the time love or life is .2 and about life 20 and again that's a simplified example allows you to do is to determine based on a random email who is the likely person who sent this email says an email that goes as full of life and you don't know send it but he'd like to figure that out then you can do the space on base for and suppose you believe a priori that its 50% probability by Chris or vasara service AP of Chris equals open 5/3 means the prior probability for increases 50% and then immediately means because of the to the probability of Sarah is out of Europe so if you look at this dude if you're likely to have written this email Chris or Sarah one box 

it's Thursday May 11th and in case you missed it artificial intelligence is a big deal these days whatever happened Gadget maker looking to pack ever more smarts into their products typically researchers working on artificial intelligence make it smarter by using what's known as machine learning and since that usually involves computers teaching computers as it worth the faster your computer is the better however one specific kind of computer the high-powered do you or Graphics Processing Unit which originally was developed in response to demands for high-end Graphics in video games seems especially well-suited to machine learning there's no bigger name in the GPU game then Nvidia now Invidia is putting more focus on making products specifically for AI and they've just introduced a new machine learning GPU called the Tesla p100 let's just say right now that the V100 is huge it's very very expensive and it's not for gamers it's for day centers in AI workouts but take heart this kind of stuff tends to trickle down quickly will definitely see a gaming GPU based on the new text sooner than later you can read more start Nvidia gpus Ai and machine learning at the link if the word key logging doesn't strike fear in your heart well it should and now a security firm says they discovered eki logging program on some HP computers eki logging program does just that record every keystroke you make on your computer and stores it Zachary or malware program gets ahold of the file they can use it to suss out passwords private information logins and much more Swiss cybersecurity firm he wrote says they found a key locking program inside an audio driver of all things on numerous HP laptop including the new folio G1 elitebook Pro book and zbook models we've got to link to a complete list of affected PCS and there's a fixed removing the key lock you program also at the link its iPhone 8 rumors have time again and this time it's not so much what the phone will be like but when it'll arrive and how many there will be according to a post on BGR reliable Apple secretly only says the new top-of-the-line iPhone whatever it ends up being called will indeed be ready to be revealed and shipped in September despite Rumours the phone was being delayed due to delicious how many phones will be available is another issue altogether of course meanwhile Forbes is saying their sources are saying Apple May introduce for new iPhones September 4th phone a revamp of the surprise hit iPhone SE which will be called the se2 or SE Series 2 and it won't look like fratesi which retains the Steve Edition design with sharp edges and a small screen the next as he will be smaller than the iPhone 7S or 8 but according Forbes old edges will be rounded down to better match the form factor of the rest of the line again it's all rumors and speculation until Tim Cook takes the stage just 5 short months from now have a great summer that's it for DT daily today get more Tech news updates on her Facebook page and YouTube channel we sure to check out our Roundtable podcast trans with benefits live at 2:30 p.m. today thanks watching back again tomorrow 

for decades the top Banks retailers insurance companies and governments have light on IBM Z systems for the scalability performance integrity and security they demand IBM Z systems is there on premise private Cloud platform capable of processing billions of business critical transactions per second with its ruggedness Shear computing power Z systems ranks as the world's most reliable server for the 8th straight year but even with a full art Advanced analytics tools data science teams still face a constant battle to extract deeper insights from a rich fast-moving sea of data with machine-learning available for z/os and built on Apache spark organizations can develop test tips for cognitive applications with unmatched speed all unified with z for the data that drives the Enterprise machine learning converges IBM cutting-edge research and analytics Technologies with Source software to create a simple powerful experience that Fosters collaboration across personas machine running combined with this transactional data creates true ruler named machines with models that adapt InTune themselves to achieve smarter outcomes IBM gives you a unique advantage over the competition to submit deep personal relationships with your customers identify new business opportunities and minimize risk and fraud skilled data scientists need smart date systems and IBM brings you the full capabilities of its machine Learning Without compromise without the need to move or expose your that's a risk the result optimal intelligence performance scalability and Rock Solid security for you your customers click the link below to see machine learning in action realize your cognitive potential 

we all know that machine learning can do some pretty amazing things with data uncover hidden insights fix unfixable problems predict the future but it Microsoft We Believe machine learning can be even more amazing by being accessible to all so we created as your learning machine learning in the cloud that's easier to use how easy it all begins with the setting it up heart you never have to ask anyone to provision your workspace even if you're using a desktop all you need is a web browser log on and you're good to go next comes building your model where you'll really benefit from Azure ml jumped in flexibility with one click you can land on the algorithm that's best for your business whether that's are python or battle-tested Microsoft algorithm run businesses like Xbox and Bing you can even add your own Special Sauce by bringing in custom r or python code then there's the really fun will you get to put your creation to work as your ml is designed for Applied machine learning which means that in minutes your model is deployed as a manage web service that can connect to any data anywhere that's when the full power of machine learning is truly Unleashed and as users syrians have power and wonder what else it can do you can easily update your solution and put it back into production was still being able to revisit your old results and on Global Azure machine learning Marketplace you can expand your reach by creating your own brand and business model and with Microsoft cloud take advantage of unmatch scalability availability and security Azure machine learning from Microsoft easy to build easy to deploy easy to share so you thought machine learning couldn't get any more amazing 



one very powerful place that you can use regular ization is in regression regularization is a method for automatically penalizing the extra features that you use and your model so let me make this a little bit more concrete there's a type of regular eyes regression called lasso regression and here's the rough formula for a lasso regression a regular linear regression with say I just want to minimize the sum of the squared errors in My Fit I want to minimize the distance between my and any given data point or the square of that distance what lasso regression says is yeah we want a small sum of squared error but addition to minimizing the sum of the squared errors I also want to minimize the number of features that I'm using as I'm going to add in a second term here in which I have a T parameter and I have the coefficients of my regression so this is basically the term that describes how many features on using so here's the result of this formulation when I'm performing My Fit I'm considering both the errors that come from that fit and also the number of features that are being used until the same I'm two different fits that have different number of features in them the one that has more features included will almost certainly have a smaller sum of the squared error because I can if it more precisely to the points but I pay a penalty for using that extra feature and that comes in the second term with the with a penalty term and the never aggression that I'm going to get for that additional feature that I'm using and so what this is saying is it the game that I get in terms of the the Precision the goodness of fit regression has to be a bigger game than the the last that I take as a result of having that additional feature in my regression so this precise play formulates in a mathematical way the trade-off between having small errors and having a simpler fit that's using fewer features and so what last over does is it automatically takes into account this penalty perimeter and in so doing it helps you actually figure out which features are the ones that have the most important effect on your regression and once it's found those features it can actually eliminate or set to zero the coefficients for the features that basically don't help 

your dance on Meloni algorithm we're done Michael we're done with the entire lesson we don't have to do anything else anymore we know there was supposed to look over subset of data pick up rules and then combine them so what else do you need to go in order to write your first Ensemble learning out really dumb ways to combine is one it random or you know I don't know Adam all up and divided by pi I mean so so presumably there's got to be some intelligent how this combination is taking place but you're not at all bothered about how you pick us up set so you already mated sumption but how we're going to pick us up set you just want your of the combine them will actually let sit for a minute here's kind of the dumbest thing you can imagine doing that turns out to work pretty well we're going to pick subsets by I'm going to say uniformly just to be specific about it so we're going to do the dumbest thing that we could thing that's one of the dumbest things you can think of or maybe we should say simplest and not dumbest so as not to to make a value judgment that you can think of doing which would be there just uniformly in the morning Shoes Among some of the data and say that's the day that I'm going to look at it and then I'm going to play some running out of rhythm to it that what you're thinking of Michael okay so just pick us up some of the data Lerner to it I'll get some hypothesis out I'll get some rule out and I'm going to combine them so since we're being simple you try doing something simple for combining let's imagine Michael that we're doing regression what's kind of the simplest thing you could do if you have 10 different rules which tell you how you should be predicting some new data point what's the simplest thing you can imagine doing with them so okay so each of them spits out a number I guess if we kind of equally believe can each of them a reasonable thing to do would be to average so a very simple way of combining in the case of regression would be to average them will simply take the meme and by the why wouldn't we equally believe in each of them each one of them learned over a random subset of the data you have no reason to take any one's better than the other add random subset I don't know how I would measure that could be a good random subset you know it gets more error than others or uses them more komplex rule than others or something I can actually maybe we can explore how the sort of idea might go wrong do that baby girl with the quiz you like quizzes right there important 

hello in this video will be covering languages type an examples machine learning is an algorithm that can learn from data without being relying on standard programming practices like object Orient design the purpose of machine learning is to make machines more self-sufficient insofar as they're able to analyze data and recognize patterns what are the benefits of machine learning is that it can be used to automate processes and make them more efficient there are actually a wide range of technologies that in bread machine learning many of which you may use in your daily life a subset or algorithm of machine learning known as regression do use to interpret label data to predict Trends and think such as house pricing which may produce faster more accurate predictions than modern statistical modeling in this course will be introducing Core Concepts to machine learning such a supervised and unsupervised learning types how would correlate Statics model evaluation and a bunch of popular algorithms machine learning uses two primary languages Python and R as well as Java C C++ Matlab Haskell and list thanks for watching 

information theory has a very interesting History Channel was a genius mathematician who was working at Bell Labs who came out with this information theory is also called as the father of Farmington age why it is interesting is because at the time but Labs had a communication mechanism set up and I just figured out long-distance communication but they had no idea how to charge people so you could send a message and it would charge you per message or they could find out how many words are in those message and they could charge you part world but none of them made sense because you could sometimes write shorter sentences and convey much more information so it really became necessary to find out what is information and when was the first person to ever try to work on that problem and figure something out but information theory has also a background from physics and that is why is words like entropy in it the physicist who studied thermodynamics where the first scientists to actually understood information if you're interested in more about the physicist background information Theory you should read up on Maxwell's demon Mac system in is a very famous products pyramid Maxwell came out with in physics we believe that energy can neither be created or destroyed but maximals demon proves that energy can be converted into information and the common turn off energy and information can neither be created or destroyed but let's come back to the big world and let's discuss how Clarkson and looked at it so his task was to send messages from one place to the other and try to figure out which message has more information so let's start with a simple example 

to extend the learning rule for a Lenny in urine to learning will we can use for multi Line-X of nominee neurons we need 2 stamps first we need to extend the learning room to a single nonlinear neuron and we're going to use logistic guns will and many other kinds of non-linear Orleans could Houston standing rule for Lenny in urine to a logistic neuron which is unknown neurologist ignoring computes it's legit Z which is his total input it's his bias Plus remove roll it simple lines of the value of an unknown import Line-X I times the wait on that line wi you think is not why this is smooth known many a function of that legit ashern in the garage here. Function is approximately 0 Wednesdays begin approximately 1 wins is big and positive and in between it changes smoothly and nonlinearly the fact that it changes continuous gives it nice to riveted switchmate Learning Easy so you get the derivative of a logistic neuron with respect to the whites which is what we need for luning we first need to compute the derivative of the legend itself that is the total import with respect to await that's very simple the logic is just a bias plus the sum of rollie input lines of the value on the input line time play wait so when we differentiate with respect to wi we just got access to the loves you with respect to wxxi and simile the derivative of allergic respect to X is wi the derivative of the output with respect to the loge it is also simple if you expressed in terms of the I put so I put his one of them plus each of the minus Z and Dy by disease just why into 1-1 that's not obvious for those of you who like to see the math I put it on Slide the math is tedious but perfectly straightforward so you can go through it by yourself no weave feed the root of the iPod with respect to the Loggia on the route of the legit with respect to the wait we can start to figure out the derivative of the iPod with respect to the we just use the chain rule again so do I buy TW is dizi by DW x / dizzy and dizzy buddy W as we just saw a sexy do I buy disease is wine - 1 - y + so we now have are you learning rule for a logistic neuron we've got the Wi-Fi DW wrong we need to do is use the chain rule once more I multiply it by and we got something that looks very like the Delta rule so the way the error changes as we change your weight te by DWI is just as some of the training cases end of the value on import Line-X in planes the residual the difference between the Target and the I put on the actual I put it in your own but it's got this extra toe that which comes from the slope of the logistic function which is why I'm into 1-1 so slight motive mission of adultery gives us the gradient descent learning rule for training and logistic unit 

I'm currently working in the field of analytics and I keep hearing machine learning and its applications what's it all about machine learning is a field of artificial intelligence they were first to the ability of certain programs to learn and grow with data a simple example would be a Spam classifier which used by most email clients spam classifiers can be I need to differentiate between spam and a non spam emails which can then be used to filter out spam emails elsewhere machine learning is all the rage these days and is a very important skill to have if you work in the analytics industry what are the most famous example is Google itself it's a machine learning recommendation engine which process is a huge number of inputs to bring you the most results if you're into social media then Facebook's news feed and friend suggestion algorithms are other good examples Amazon and Netflix is Wrecking patient systems which have brought Great Value to both customers and the companies themselves are well-known applications of machine learning all these applications are learn improve over time with more user data available making better predictions in the process great so what's working in the industry like I consider a machine learning can companies like Amazon Google Microsoft and Apple are hiring aggressively for machine learning specialist machine learning Engineers John average salary $115,000 a year with Rising demand and steep salary hikes it's one of the hottest fields around so how do I begin or the best way to what a machine learning career is with a good training course check out Simply learn to machine learning certification training it takes a unique case study based approach training ensuring that you get practical implementation exposure even as you study Theory it covers all the techniques of supervised and unsupervised learning putting deep learning and Spark machine learning the course also includes quizzes and projects to reinforce your learning and over 28 hours of instructor online training with Cloud lab integration you have a hassle-free experimentation environment to practice I will register for the training right away 



hi I'm rich with inside HBC we're here at sc16 in Salt Lake City and this morning wearing at the Intel resume with Kevin welcome to Salt Lake City sir thank you question about this dumb all this is on the fpga is right what's it showing us sure convolutional neural networks being accelerated on fpga using are opencl high-level design tools and we're implementing the Alex next CNN to Pieology and showing the acceleration of image classification using the imagenet data set call machine-learning there's the two sides right we have the inference side and the training side which one is this this is so we're taking a pre-trained row and we are using those weights from the pre train model to do the classification what are the advantages of fpga when you're doing this side of it is it speed is a data movement what's what's going on here and power efficiency so if you see up on the top left were showing a little over a images for second at a power envelope of 40 to 45 watts and user take advantage of this with with their workflow and how they get work done so we implemented this using our opencl design tools basically we design the whole network in opencl and compile it to fpga from an end users respective we're tying this together with mkl DNN and then Cafe on top of that and Intel announced this week a product called Eli deep learning inference accelerator which is basically this entire solution in a box 

sleep Tom Tom runs a local pet store business is going okay and the store keeps attracting new customers but he's noticed that some of his existing I have been spending less or even stopped purchasing completely Tom would really like to spot these customers before they start churning so we can convince them to stick around with the map of customer churn prediction service by Andrew machine learning Tom is able to do just that by analyzing Tom sales history data using the power of machine learning the service identifies customers that are most likely to churn Tom went online to check it out and in less than an hour he had a list of all this customers 
so I'm learning from text one of the fundamental problems we have is that the length of each document do you learn from each email or each book title is non-standardized so you can't just use each into the word as they able to feature because then long emails would require different input space than short email instead one of the coolest things in machine learning from text analyze all these approaches is called bag of words in the basic idea is to take any texts and count the frequency of Words which you do for me in a second so check dimples dictionary see if all the votes you know officer things to divorce like nice very day am I to voice it on the left side like he she love to say these other boys you care about notices fun on the left side doesn't even occur on the right side then you can map each Trace email each title each text into a frequency count Define over these six features of a useless I give you 6 boxes for each of the six words in a dictionary of all the ones you care about can you for the May 1st since nice day filling the frequency count of those words in this text and give me that Vector so particular which what are we up you tell me how often are cruise in nice day 
